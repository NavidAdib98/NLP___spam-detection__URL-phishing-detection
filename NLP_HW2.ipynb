{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>ØªÙ…Ø±ÛŒÙ† Û²<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1>\n",
    "<h2 style=\"color:rgb(90, 255, 184); font-size: 20px;\">Logistic Regression and Naive Bayes</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">ÙØ±Ù‡Ø§Ø¯ Ù†ØµØ±ÛŒ - Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø²Ù…Ø§Ù†ÛŒ</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">farhadnasri999@gmail.com - shigzv@gmail.com</p>\n",
    "\n",
    "<div dir=\"rtl\" style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">ğŸ“ Ù…Ø´Ø®ØµØ§Øª Ø¯Ø§Ù†Ø´Ø¬Ùˆ:</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ: Ù†ÙˆÛŒØ¯ Ø§Ø¯ÛŒØ¨</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ: Û¸Û±Û°Û±Û°Û´Û°Û°Û´</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ØªØ§Ø±ÛŒØ® Ø§Ø±Ø³Ø§Ù„: {{ØªØ§Ø±ÛŒØ®_Ø§Ø±Ø³Ø§Ù„}}</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<div style=\"padding-right:100px\">\n",
    "ğŸ“‹ <b>Ø³Ø§Ø®ØªØ§Ø± ØªÙ…Ø±ÛŒÙ†:</b>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"ltr\">Logistic Regression and Naive Bayes (from scratch)</span> (60)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Bag of Words</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression</li>\n",
    "<li>Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Naive Bayes</li>\n",
    "<li>Ø¨Ø®Ø´ Ø´Ø´Ù…: ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"ltr\">Logistic Regression and Naive Bayes (e.g. \n",
    "sklearn)</span> (40)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "ğŸ’¡ <b>Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</b>\n",
    "<br>\n",
    "Ø¯Ø± Ø³ÙˆØ§Ù„ Ø¯Ùˆ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ Ù‡Ø³ØªÛŒØ¯ ÙˆÙ„ÛŒ Ø¯Ø± Ø³ÙˆØ§Ù„ ÛŒÚ©ØŒ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ù…ØªÙ† Ø³ÙˆØ§Ù„ Ù‡Ù… Ø°Ú©Ø± Ø´Ø¯Ù‡ØŒ Ø¨Ø§ÛŒØ¯ Ø§Ø² Ù¾Ø§ÛŒÙ‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression Ùˆ Naive Bayes <br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ† Ø¯Ùˆ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙˆÙ† Ø±Ø§ Ø§Ø² ØµÙØ± (Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡) Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯. Ù‡Ù…â€ŒÚ†Ù†ÛŒÙ† Ù†ÛŒØ§Ø² Ø§Ø³ØªØŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ù†ÛŒØ² Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Bag of Words</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ú¯Ø§Ù… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ØŒ Ù‡Ø¯Ù Ù…Ø§ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø§Ø³Øª. Ø§Ø² Ø¢Ù†â€ŒØ¬Ø§ Ú©Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ù…ØªÙ† Ø®Ø§Ù… Ø±Ø§ Ø¨Ù‡ Ø´Ú©Ù„ÛŒ Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ… ØªØ§ Ø¨ØªÙˆØ§Ù†Ù†Ø¯ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨ÛŒØ§Ù…ÙˆØ²Ù†Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ Ù‚ØµØ¯ Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø§ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙˆÙ† Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Bag of WordsØŒ Ù‡Ø± Ø§ÛŒÙ…ÛŒÙ„ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒØ´ Ø¨Ù‡ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ….\n",
    "<br>\n",
    "Ø¯ÛŒØªØ§Ø³Øª emails_1.csv Ø´Ø§Ù…Ù„ Ø¯Ùˆ Ø³ØªÙˆÙ† Ø§Ø³Øª:\n",
    "<br>\n",
    "text (Ù…ØªÙ† Ø§ÛŒÙ…ÛŒÙ„)\n",
    "<br>\n",
    "status (Ø¨Ø±Ú†Ø³Ø¨ØŒ Ù…Ø´Ø®Øµâ€ŒÚ©Ù†Ù†Ø¯Ù‡â€ŒÛŒ spam ÛŒØ§ ham Ø¨ÙˆØ¯Ù† Ø§ÛŒÙ…ÛŒÙ„)\n",
    "<br>\n",
    "Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ù…Ø±Ø§Ø­Ù„ Ø²ÛŒØ± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:\n",
    "<br>\n",
    "Û±. Ø³ØªÙˆÙ† status Ø±Ø§ Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯ (spam â†’ 1 Ùˆ ham â†’ 0).\n",
    "<br>\n",
    "Û². Ù…ØªÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯: ØªÙ…Ø§Ù… Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ± Ø§Ù„ÙØ¨Ø§ÛŒÛŒ (Ø§Ø¹Ø¯Ø§Ø¯ØŒ Ø¹Ù„Ø§Ø¦Ù… Ùˆ ØºÛŒØ±Ù‡) Ø±Ø§ Ø­Ø°Ù Ùˆ Ù‡Ù…Ù‡â€ŒÛŒ Ø­Ø±ÙˆÙ Ø±Ø§ Ú©ÙˆÚ†Ú© (lowercase) Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Û³. Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ Bag of WordsØŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ÙÙ‚Ø· Û±Ûµ Ú©Ù„Ù…Ù‡â€ŒÛŒ Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡ Ù†Ú©ØªÙ‡: Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² CountVectorizer Ø¯Ø± Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÛŒ sklearn Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "ÛŒÚ© Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¬Ø¯ÛŒØ¯ Ú©Ù‡ Ø´Ø§Ù…Ù„ Û±Ûµ ÙˆÛŒÚ˜Ú¯ÛŒ + Ø³ØªÙˆÙ† status Ø§Ø³Øª. (ØªÙ…Ø§Ù… Û±Û° Ø±Ø¯ÛŒÙ Ø§ÛŒÙ† Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯.)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ‰ Congratulations! You have won a FREE ticket ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reminder: The project meeting will be held tom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  status\n",
       "0  ğŸ‰ Congratulations! You have won a FREE ticket ...       1\n",
       "1  Reminder: The project meeting will be held tom...       0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load the dataset\n",
    "df = pd.read_csv(\"datasets/q1/emails_1.csv\")\n",
    "# Step 3: Convert labels to numeric values (spam â†’ 1, ham â†’ 0)\n",
    "df['status'] = df['status'].map({'spam': 1, 'ham': 0})\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>status</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ‰ Congratulations! You have won a FREE ticket ...</td>\n",
       "      <td>1</td>\n",
       "      <td>congratulations you have won a free ticket to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reminder: The project meeting will be held tom...</td>\n",
       "      <td>0</td>\n",
       "      <td>reminder the project meeting will be held tomo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  status  \\\n",
       "0  ğŸ‰ Congratulations! You have won a FREE ticket ...       1   \n",
       "1  Reminder: The project meeting will be held tom...       0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  congratulations you have won a free ticket to ...  \n",
       "1  reminder the project meeting will be held tomo...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Define a text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove all non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Step 5: Apply text cleaning to all messages\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 frequent words: ['and' 'claim' 'enjoy' 'files' 'for' 'free' 'get' 'our' 'please' 'project'\n",
      " 'the' 'ticket' 'to' 'you' 'your']\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Initialize CountVectorizer to keep only top 15 frequent words\n",
    "vectorizer = CountVectorizer(max_features=15)\n",
    "\n",
    "# Step 7: Fit and transform the clean text into numerical vectors\n",
    "X = vectorizer.fit_transform(df['clean_text']).toarray()\n",
    "\n",
    "# Step 8: Retrieve the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Top 15 frequent words:\",feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   your  free  for  the  to  you  and  claim  get  our  please  project  \\\n",
      "0     1     2    0    0   1    1    0      1    0    1       0        0   \n",
      "1     1     0    0    1   0    0    1      0    0    0       1        2   \n",
      "2     0     2    2    0   0    1    0      0    1    1       0        0   \n",
      "3     1     0    0    1   0    0    1      0    0    0       1        0   \n",
      "4     2     1    1    0   0    1    0      1    0    0       0        0   \n",
      "5     0     1    0    0   0    0    0      0    1    1       0        0   \n",
      "6     0     0    1    1   0    0    0      0    0    0       1        1   \n",
      "7     2     1    0    1   1    0    0      1    0    0       0        0   \n",
      "8     1     0    1    1   1    1    1      0    1    0       0        0   \n",
      "9     2     0    1    0   1    0    0      0    0    0       0        0   \n",
      "\n",
      "   enjoy  files  ticket  status  \n",
      "0      0      0       2       1  \n",
      "1      0      1       0       0  \n",
      "2      1      0       0       1  \n",
      "3      0      0       0       0  \n",
      "4      1      0       0       1  \n",
      "5      0      0       0       1  \n",
      "6      0      1       0       0  \n",
      "7      0      0       0       1  \n",
      "8      0      0       0       0  \n",
      "9      0      0       0       1  \n"
     ]
    }
   ],
   "source": [
    "# Step 9: Compute total frequency of each word\n",
    "word_counts = X.sum(axis=0)\n",
    "\n",
    "# Step 10: Pair each word with its total count\n",
    "word_freq = [(feature_names[i], word_counts[i]) for i in range(len(feature_names))]\n",
    "\n",
    "# Step 11: Sort words by count (descending)\n",
    "sorted_words = [word for word, count in sorted(word_freq, key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "# Step 12: Create the Bag of Words DataFrame using the sorted column order\n",
    "bow_df = pd.DataFrame(X, columns=feature_names)\n",
    "bow_df = bow_df[sorted_words]   # reorder columns\n",
    "\n",
    "# Step 13: Add the target column (status) as the last column\n",
    "bow_df['status'] = df['status'].values\n",
    "\n",
    "# Step 14: Display the first few rows of the final DataFrame\n",
    "print(bow_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§Ø² Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ø¨Ø¹Ø¯ØŒ Ø¯ÛŒÚ¯Ø± Ø¨Ø§ ÙØ§ÛŒÙ„ emails_1.csv Ùˆ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø­Ø§ØµÙ„ Ø§Ø² Ø¢Ù† Ú©Ø§Ø±ÛŒ Ù†Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø´Øª. Ø¨Ø±Ø§ÛŒ Ø³Ù‡ÙˆÙ„Øª Ú©Ø§Ø±ØŒ ÙØ§ÛŒÙ„ emails_2.csv Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³Øª. Ø§ÛŒÙ† ÙØ§ÛŒÙ„ØŒ Ù†ØªÛŒØ¬Ù‡â€ŒÛŒ Ù‡Ù…Ø§Ù† ÙØ±Ø§ÛŒÙ†Ø¯ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø± Ø±ÙˆÛŒ ÛµÛ±Û·Û² Ø§ÛŒÙ…ÛŒÙ„ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª. Ø¨Ù‡ Ø¨ÛŒØ§Ù† Ø³Ø§Ø¯Ù‡â€ŒØªØ±ØŒ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ Ù‡Ø± Ø±Ø¯ÛŒÙ Ù†Ù…Ø§ÛŒØ§Ù†Ú¯Ø± ÛŒÚ© Ø§ÛŒÙ…ÛŒÙ„ Ùˆ Ù‡Ø± Ø³ØªÙˆÙ† Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡â€ŒÛŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ ÛŒÚ©ÛŒ Ø§Ø² Û³Û°Û°Û° Ú©Ù„Ù…Ù‡â€ŒÛŒ Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ø¯Ø± Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø³Øª.\n",
    "Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ Ø¨Ø§ÛŒØ¯ Ø±ÙˆÛŒ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ ÙØ§ÛŒÙ„ emails_2.csv Ø±Ø§ Ø¨Ø®ÙˆØ§Ù†ÛŒØ¯ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù† Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯ ØªØ§ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡ Ø¢Ø´Ù†Ø§ Ø´ÙˆÛŒØ¯.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù¾Ù†Ø¬ Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù…Ø°Ú©ÙˆØ± Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Email No.  the  to  ect  and  for  of    a  you  hou  ...  connevey  jay  \\\n",
      "0   Email 1    0   0    1    0    0   0    2    0    0  ...         0    0   \n",
      "1   Email 2    8  13   24    6    6   2  102    1   27  ...         0    0   \n",
      "2   Email 3    0   0    1    0    0   0    8    0    0  ...         0    0   \n",
      "3   Email 4    0   5   22    0    5   1   51    2   10  ...         0    0   \n",
      "4   Email 5    7   6   17    1    5   2   57    0    9  ...         0    0   \n",
      "\n",
      "   valued  lay  infrastructure  military  allowing  ff  dry  Prediction  \n",
      "0       0    0               0         0         0   0    0           0  \n",
      "1       0    0               0         0         0   1    0           0  \n",
      "2       0    0               0         0         0   0    0           0  \n",
      "3       0    0               0         0         0   0    0           0  \n",
      "4       0    0               0         0         0   1    0           0  \n",
      "\n",
      "[5 rows x 3002 columns]\n",
      "\n",
      "Label distribution:\n",
      "Prediction\n",
      "0    3672\n",
      "1    1500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"datasets/q1/emails_2.csv\")\n",
    "# Display first 5 rows\n",
    "print(df.head())\n",
    "# Remove the non-feature column \"Email No.\"\n",
    "df = df.drop(columns=[\"Email No.\"])\n",
    "# Check label distribution in the \"Prediction\" column\n",
    "label_counts = df[\"Prediction\"].value_counts()\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "\n",
    "### ğŸ“„ Dataset Summary\n",
    "\n",
    "- The dataset contains **5172 emails** with **3002 columns**.  \n",
    "- After removing the non-feature column **`Email No.`**, we have:\n",
    "  - **3000 word-count features**\n",
    "  - **1 label column (`Prediction`)**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Label Distribution\n",
    "\n",
    "| Label | Meaning | Count |\n",
    "|-------|---------|--------|\n",
    "| **0** | Ham     | **3672** |\n",
    "| **1** | Spam    | **1500** |\n",
    "\n",
    "- The dataset is **moderately imbalanced**, with ~71% ham and ~29% spam.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ù†Ø³Ø¨Øª Û¸Û° Ø¯Ø±ØµØ¯ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Û²Û° Ø¯Ø±ØµØ¯ Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†ÛŒØ¯. Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "X_train, X_test, y_train, y_test<br>\n",
    "ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ X_train Ùˆ X_test \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stratified_train_test_split(X, y, test_size=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Get indices for each class\n",
    "    idx_ham  = np.where(y == 0)[0]\n",
    "    idx_spam = np.where(y == 1)[0]\n",
    "\n",
    "    # Shuffle\n",
    "    np.random.shuffle(idx_ham)\n",
    "    np.random.shuffle(idx_spam)\n",
    "\n",
    "    # Test set sizes (stratified)\n",
    "    n_test_ham  = int(len(idx_ham) * test_size)\n",
    "    n_test_spam = int(len(idx_spam) * test_size)\n",
    "\n",
    "    # Split ham\n",
    "    test_idx_ham  = idx_ham[:n_test_ham]\n",
    "    train_idx_ham = idx_ham[n_test_ham:]\n",
    "\n",
    "    # Split spam\n",
    "    test_idx_spam  = idx_spam[:n_test_spam]\n",
    "    train_idx_spam = idx_spam[n_test_spam:]\n",
    "\n",
    "    # Combine train/test indices\n",
    "    train_indices = np.concatenate([train_idx_ham, train_idx_spam])\n",
    "    test_indices  = np.concatenate([test_idx_ham,  test_idx_spam])\n",
    "\n",
    "    # Shuffle final sets\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    # Return in the order you want\n",
    "    X_train = X[train_indices]\n",
    "    X_test  = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test  = y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4138\n",
      "Test size: 1034\n",
      "\n",
      "Train distribution:\n",
      "0    2938\n",
      "1    1200\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test distribution:\n",
      "0    734\n",
      "1    300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=[\"Prediction\"]).values   # convert to numpy\n",
    "y = df[\"Prediction\"].values\n",
    "X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Show results\n",
    "print(\"Train size:\", len(y_train))\n",
    "print(\"Test size:\", len(y_test))\n",
    "\n",
    "print(\"\\nTrain distribution:\")\n",
    "print(pd.DataFrame(y_train).value_counts())\n",
    "\n",
    "print(\"\\nTest distribution:\")\n",
    "print(pd.DataFrame(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: ltr; text-align: left; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "# Train-Test Split and Label Distribution\n",
    "\n",
    "The dataset contains **imbalanced labels** (ham vs spam).  \n",
    "To ensure that both training and test sets reflect the original class distribution, we used a **stratified split** with **80% for training** and **20% for testing**.\n",
    "\n",
    "- **Train size:** 4138  \n",
    "- **Test size:** 1034  \n",
    "\n",
    "**Train distribution:**  \n",
    "```\n",
    "0 (ham): 2938  \n",
    "1 (spam): 1200\n",
    "```\n",
    "\n",
    "**Test distribution:**  \n",
    "```\n",
    "0 (ham): 734  \n",
    "1 (spam): 300\n",
    "```\n",
    "\n",
    "This stratification helps models learn effectively from both classes and ensures that evaluation metrics are reliable.\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù¾ÛŒØ´ Ø§Ø² Ø¢Ù†â€ŒÚ©Ù‡ ÙˆØ§Ø±Ø¯ Ù…Ø±Ø­Ù„Ù‡â€ŒÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´ÙˆÛŒÙ…ØŒ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ù†Ø¬Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ ØªØ¹Ø±ÛŒÙ Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ø¨Ù‡ Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ ØªØ§ Ø¨ÙÙ‡Ù…ÛŒÙ… Ù…Ø¯Ù„ ØªØ§ Ú†Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø¯Ø±Ø³Øª Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ Ù…ÙˆÙÙ‚ Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.\n",
    "<br>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ ØªÙˆØ§Ø¨Ø¹ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Ø¯ÙˆØ¯ÙˆÛŒÛŒ (binary classification) Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "accuracy(y_true, y_pred) â€” Ù†Ø³Ø¨Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø³Øª Ø¨Ù‡ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§\n",
    "<br>\n",
    "precision(y_true, y_pred) â€” Ø¯Ø±ØµØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù…Ø«Ø¨Øª Ø¨ÙˆØ¯Ù‡â€ŒØ§Ù†Ø¯\n",
    "<br>\n",
    "recall(y_true, y_pred) â€” Ø¯Ø±ØµØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª ÙˆØ§Ù‚Ø¹ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø±Ø³Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª\n",
    "<br>\n",
    "f1_score(y_true, y_pred) â€” Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ø§Ø±Ù…ÙˆÙ†ÛŒÚ© Ø¨ÛŒÙ† precision Ùˆ recallØŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ ØªÙˆØ§Ø²Ù† Ù…ÛŒØ§Ù† Ø¢Ù† Ø¯Ùˆ\n",
    "<br>\n",
    "Ù‡Ø± ØªØ§Ø¨Ø¹ Ø¨Ø§ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ Ù…ØªÙ†Ø§Ø¸Ø± Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø± Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯.\n",
    "<br>\n",
    " Ø¨Ù‡ ØªÙˆØ§Ø¨Ø¹ Ø¨Ø§Ù„Ø§ Ø¯Ùˆ ÙˆØ±ÙˆØ¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø¯Ù‡ÛŒØ¯ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯:    y_true = [0, 1, 1, 0, 1] ---- y_pred = [0, 1, 0, 0, 1]\n",
    "<br>\n",
    "ğŸ’¡ Ù†Ú©ØªÙ‡: Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø²  Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÛŒ numpy Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of metrics\n",
    "import numpy as np\n",
    "\n",
    "# Calculate accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    # Proportion of correct predictions\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# Calculate precision\n",
    "def precision(y_true, y_pred):\n",
    "    # TP = predicted 1 and true 1\n",
    "    # FP = predicted 1 but true 0\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    \n",
    "    if tp + fp == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "# Calculate recall\n",
    "def recall(y_true, y_pred):\n",
    "    # TP = predicted 1 and true 1\n",
    "    # FN = predicted 0 but true 1\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    \n",
    "    if tp + fn == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "# Calculate F1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    # Harmonic mean of precision and recall\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    \n",
    "    if p + r == 0:\n",
    "        return 0.0\n",
    "    return 2 * (p * r) / (p + r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø«Ø§Ù„:\n",
    "<br>\n",
    "y_true = [0, 1, 1, 0, 1]\n",
    "<br>\n",
    "y_pred = [0, 1, 0, 0, 1]\n",
    "<br>\n",
    "print(accuracy(y_true, y_pred))  # 0.80\n",
    "<br>\n",
    "print(precision(y_true, y_pred)) # 1.00\n",
    "<br>\n",
    "print(recall(y_true, y_pred))    # 0.66\n",
    "<br>\n",
    "print(f1_score(y_true, y_pred))  # 0.80\n",
    "<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Precision: 1.0\n",
      "Recall: 0.6666666666666666\n",
      "F1 Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "y_true = [0, 1, 1, 0, 1]\n",
    "y_pred = [0, 1, 0, 0, 1]\n",
    "print(\"Accuracy:\", accuracy(y_true, y_pred))\n",
    "print(\"Precision:\", precision(y_true, y_pred))\n",
    "print(\"Recall:\", recall(y_true, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ú©Ù„Ø§Ø³ Logistic Regression Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.Ø³Ù¾Ø³ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ train Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…ØªØ±ÛŒÚ© Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ú¯Ø²Ø§Ø±Ø´ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ø§ÛŒÙ¾Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ±â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø§Ù†Ù†Ø¯ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ùˆ ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø¹Ù‡Ø¯Ù‡ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø³Øª.\n",
    "<br>\n",
    "ğŸ’¡Ù…ÛŒØªÙˆØ§Ù†ÛŒØ¯ Ù‚Ø¨Ù„ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³ØªÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, lr=0.01, epochs=100, batch_size=32, normalize=True):\n",
    "        # Learning rate\n",
    "        self.lr = lr\n",
    "        # Number of training epochs\n",
    "        self.epochs = epochs\n",
    "        # Mini-batch size\n",
    "        self.batch_size = batch_size\n",
    "        # Whether to apply normalization\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # Will be set during fitting\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Sigmoid function\n",
    "    # --------------------------------------------\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Binary cross-entropy loss\n",
    "    # --------------------------------------------\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # Clip values for numerical stability\n",
    "        y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Forward pass\n",
    "    # --------------------------------------------\n",
    "    def forward(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.W) + self.b)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Normalize features\n",
    "    # --------------------------------------------\n",
    "    def normalize_features(self, X):\n",
    "        self.mean = X.mean(axis=0)\n",
    "        self.std = X.std(axis=0) + 1e-10  # avoid division by zero\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Apply stored normalization parameters\n",
    "    # --------------------------------------------\n",
    "    def apply_normalization(self, X):\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Training loop (SGD)\n",
    "    # --------------------------------------------\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        # Normalize if enabled\n",
    "        if self.normalize:\n",
    "            X = self.normalize_features(X)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.W = np.zeros(n_features)\n",
    "        self.b = 0.0\n",
    "\n",
    "        # Determine progress checkpoints\n",
    "        checkpoints = {int(self.epochs * p / 100) for p in range(10, 101, 10)}\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "\n",
    "            # Shuffle the data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            # Mini-batch SGD\n",
    "            for start in range(0, n_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                X_batch = X[start:end]\n",
    "                y_batch = y[start:end]\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "\n",
    "                # Gradient calculation\n",
    "                error = y_pred - y_batch\n",
    "                dW = np.dot(X_batch.T, error) / len(X_batch)\n",
    "                db = np.mean(error)\n",
    "\n",
    "                # Gradient descent update\n",
    "                self.W -= self.lr * dW\n",
    "                self.b -= self.lr * db\n",
    "\n",
    "            # Print progress at each 10%\n",
    "            if epoch in checkpoints:\n",
    "                percent = int(epoch / self.epochs * 100)\n",
    "                print(f\"Training progress: {percent}%\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Predict probability\n",
    "    # --------------------------------------------\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X)\n",
    "\n",
    "        if self.normalize:\n",
    "            X = self.apply_normalization(X)\n",
    "\n",
    "        return self.forward(X)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Predict class (0/1)\n",
    "    # --------------------------------------------\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress: 10%\n",
      "Training progress: 20%\n",
      "Training progress: 30%\n",
      "Training progress: 40%\n",
      "Training progress: 50%\n",
      "Training progress: 60%\n",
      "Training progress: 70%\n",
      "Training progress: 80%\n",
      "Training progress: 90%\n",
      "Training progress: 100%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionScratch(\n",
    "    lr=0.05,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.965183752417795\n",
      "Precision: 0.9230769230769231\n",
      "Recall: 0.96\n",
      "F1 Score: 0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy(y_test, y_pred))\n",
    "print(\"Precision:\", precision(y_test, y_pred))\n",
    "print(\"Recall:\", recall(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: ltr; text-align: left; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "\n",
    "\n",
    "# Logistic Regression (Summary)\n",
    "\n",
    "**Logistic Regression** is a supervised **binary classification** algorithm. It models the probability that an input belongs to the positive class.\n",
    "\n",
    "## Model\n",
    "The model computes:\n",
    "\n",
    "$$\n",
    "z = W^T X + b\n",
    "$$\n",
    "\n",
    "and applies the **sigmoid** function:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "which outputs a probability between **0 and 1**.\n",
    "\n",
    "## Loss Function\n",
    "Training uses **Binary Cross-Entropy**:\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\Big]\n",
    "$$\n",
    "\n",
    "## Optimization\n",
    "Parameters are updated using **gradient descent**:\n",
    "\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial L}{\\partial W}, \\quad\n",
    "b := b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "## Prediction\n",
    "Predict **1** if \n",
    "\n",
    "$$\n",
    "\\hat{y} \\ge 0.5\n",
    "$$\n",
    "\n",
    "otherwise **0**.\n",
    "\n",
    "## Why Use It?\n",
    "- Simple and fast  \n",
    "- Interpretable  \n",
    "- Produces calibrated probabilities\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Naive Bayes</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ú©Ù„Ø§Ø³ Multinomial Naive Bayes Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯. Ø³Ù¾Ø³ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ train Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø± Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ú¯Ø²Ø§Ø±Ø´ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ú†Ø±Ø§ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ù…Ø§Ù†Ù†Ø¯ Gaussian Naive Bayes Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ø±Ø¯ÛŒÙ…ØŸ Ø¢ÛŒØ§ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Naive Bayes Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ØŸ\n",
    "<br>\n",
    "ğŸ’¡Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³ØªÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ø¨Ø±Ø§ÛŒ Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø¨Ø§ Naive Bayes Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ Appendix Ú©ØªØ§Ø¨ jurafsky Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, alpha=1.0, normalize=False):\n",
    "        \"\"\"\n",
    "        alpha: Laplace smoothing\n",
    "        normalize: if True, normalize features before training (not needed for MNB)\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        self.class_log_priors = None\n",
    "        self.feature_log_probs = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        \n",
    "    # --------------------------------------------\n",
    "    # Normalize features (optional, rarely needed)\n",
    "    # --------------------------------------------\n",
    "    def normalize_features(self, X):\n",
    "        self.mean = X.mean(axis=0)\n",
    "        self.std = X.std(axis=0) + 1e-10\n",
    "        return (X - self.mean) / self.std\n",
    "    \n",
    "    def apply_normalization(self, X):\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Fit the Multinomial Naive Bayes model\n",
    "    # --------------------------------------------\n",
    "    def fit(self, X, y, epochs=1):\n",
    "        \"\"\"\n",
    "        Since NB is a closed-form algorithm, epochs=1 is enough.\n",
    "        But added epochs only for 10% progress reporting.\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Optional normalization\n",
    "        if self.normalize:\n",
    "            X = self.normalize_features(X)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        classes = np.unique(y)\n",
    "        n_classes = len(classes)\n",
    "\n",
    "        # Initialize matrices\n",
    "        class_counts = np.zeros(n_classes)\n",
    "        feature_counts = np.zeros((n_classes, n_features))\n",
    "\n",
    "        # Progress checkpoints (10,20,...,100)\n",
    "        checkpoints = {int(epochs * p / 100) for p in range(10, 101, 10)}\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "\n",
    "            # Counting features and class frequencies\n",
    "            for idx, cls in enumerate(classes):\n",
    "                X_c = X[y == cls]\n",
    "                class_counts[idx] = X_c.shape[0]\n",
    "                feature_counts[idx] = X_c.sum(axis=0)\n",
    "\n",
    "            # Compute class prior: P(y)\n",
    "            self.class_log_priors = np.log(class_counts / n_samples)\n",
    "\n",
    "            # Compute conditional probabilities with Laplace smoothing\n",
    "            # P(word_i | class)\n",
    "            smoothed_feature_counts = feature_counts + self.alpha\n",
    "            smoothed_totals = smoothed_feature_counts.sum(axis=1).reshape(-1, 1)\n",
    "            self.feature_log_probs = np.log(smoothed_feature_counts / smoothed_totals)\n",
    "\n",
    "            # Print progress\n",
    "            if epoch in checkpoints:\n",
    "                print(f\"Training progress: {int(epoch / epochs * 100)}%\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Predict log-probabilities for each class\n",
    "    # --------------------------------------------\n",
    "    def predict_log_proba(self, X):\n",
    "        X = np.array(X)\n",
    "        if self.normalize:\n",
    "            X = self.apply_normalization(X)\n",
    "\n",
    "        # log P(y) + sum (count_i * log P(word_i | y))\n",
    "        return self.class_log_priors + np.dot(X, self.feature_log_probs.T)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Predict class labels\n",
    "    # --------------------------------------------\n",
    "    def predict(self, X):\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        return np.argmax(log_probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress: 10%\n",
      "Training progress: 20%\n",
      "Training progress: 30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress: 40%\n",
      "Training progress: 50%\n",
      "Training progress: 60%\n",
      "Training progress: 70%\n",
      "Training progress: 80%\n",
      "Training progress: 90%\n",
      "Training progress: 100%\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNaiveBayes(alpha=1)\n",
    "model.fit(X_train, y_train, epochs=10)   # epochs only for showing progress\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.937137330754352\n",
      "Precision: 0.8637770897832817\n",
      "Recall: 0.93\n",
      "F1 Score: 0.8956661316211878\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy(y_test, y_pred))\n",
    "print(\"Precision:\", precision(y_test, y_pred))\n",
    "print(\"Recall:\", recall(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: ltr; text-align: left; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "\n",
    "# Multinomial Naive Bayes (Summary)\n",
    "\n",
    "**Multinomial Naive Bayes (MNB)** is a supervised **probabilistic classifier** commonly used for **text classification** tasks like spam detection and sentiment analysis.\n",
    "\n",
    "## Model\n",
    "MNB models the probability of a class given a document using:\n",
    "\n",
    "$$\n",
    "P(y \\mid X) \\propto P(y) \\prod_{i=1}^{n} P(w_i \\mid y)^{x_i}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\(P(y)\\) = prior probability of class \\(y\\)  \n",
    "- \\(P(w_i \\mid y)\\) = probability of word \\(i\\) given class \\(y\\)  \n",
    "- \\(x_i\\) = count of word \\(i\\) in the document  \n",
    "\n",
    "The **log probabilities** are often used to avoid numerical underflow:\n",
    "\n",
    "$$\n",
    "\\log P(y \\mid X) = \\log P(y) + \\sum_{i=1}^{n} x_i \\log P(w_i \\mid y)\n",
    "$$\n",
    "\n",
    "## Training\n",
    "- Count occurrences of each word per class  \n",
    "- Apply **Laplace smoothing** to avoid zero probabilities:\n",
    "\n",
    "$$\n",
    "P(w_i \\mid y) = \\frac{\\text{count}(w_i, y) + \\alpha}{\\sum_j \\text{count}(w_j, y) + \\alpha V}\n",
    "$$\n",
    "\n",
    "Where \\(\\alpha\\) is usually 1 and \\(V\\) is vocabulary size.\n",
    "\n",
    "## Prediction\n",
    "Predict the class with the **highest posterior probability**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_y \\log P(y \\mid X)\n",
    "$$\n",
    "\n",
    "## Why Multinomial NB?\n",
    "- Works well with **count-based features** (Bag-of-Words, TF-IDF)  \n",
    "- Very fast and simple  \n",
    "- Handles sparse high-dimensional data efficiently  \n",
    "\n",
    "## Why Not Gaussian NB?\n",
    "- Gaussian NB assumes **continuous features with Gaussian distribution**  \n",
    "- Text features are **counts (integers)** and non-negative  \n",
    "- Using Gaussian NB can produce incorrect probabilities and poor performance  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"> Ø¨Ø®Ø´ Ø´Ø´Ù…: ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù†ØªØ§ÛŒØ¬ Ø¯Ùˆ Ù…Ø¯Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ÛŒ Ø¨Ø± Ù†ØªØ§ÛŒØ¬ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯.<br>\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ…: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Logistic Regression Ùˆ Naive Bayes <br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">Ù‡Ø¯Ù Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Logistic Regression Ùˆ Naive Bayes Ø¯Ø± ØªØ´Ø®ÛŒØµ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø§Ø² Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ (Ù…Ø¹ØªØ¨Ø±) Ø§Ø³Øª.\n",
    "Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÛŒ Ø§Ø±Ø§Ø¦Ù‡â€ŒØ´Ø¯Ù‡ (Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒ Ø®Ø§Ù…)ØŒ ÛŒÚ© Ø³Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø² Ø§ÛŒÙ† URLÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø§ÛŒÛŒØ¯ Ùˆ ØªØ£Ø«ÛŒØ± Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.<br>Ù†Ú©ØªÙ‡: Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨â€ŒØ®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù‡Ø³ØªÛŒØ¯.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø§ÙˆÙ„: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§Ø² Ø¢Ø¯Ø±Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø·Ù‡â€ŒÙ‡Ø§ (nb_dots)\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ù„Ø´â€ŒÙ‡Ø§ (nb_slashes)\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ø®Ø· â€ŒØªÛŒØ±Ù‡â€ŒÙ‡Ø§ (nb_hyphens)\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§ÛŒÙ† Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ø´ØªØ§Ø¯ Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´) Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ Ø¨ÛŒØ³Øª Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†) Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯\n",
    "<br>\n",
    "<br>\n",
    "- Ø¢ÛŒØ§ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ØŸ Ú†Ø±Ø§ØŸ Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø´Ù…Ø§ Â«Ø¨Ù„Ù‡Â» Ø§Ø³ØªØŒ Ù†ÙˆØ¹ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¢Ù† Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø§Ø² Ø¨ÛŒÙ† Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Naive Bayes (GaussianNB ÛŒØ§ MultinomialNB) Ú©Ø¯Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³ØªØŸ Ø¯Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ùˆ Ø¨Ø§ Ø¢Ù† Ù…Ø¯Ù„ Ø¢Ø²Ù…Ø§ÛŒØ´ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11430 entries, 0 to 11429\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     11430 non-null  object\n",
      " 1   status  11430 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 178.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"datasets/q2/urls.csv\")\n",
    "df[\"status\"].value_counts()\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label balance check\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    5715\n",
       "1    5715\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to numeric: legitimateâ†’0, phishingâ†’1\n",
    "df['label'] = df['status'].map({'legitimate': 0, 'phishing': 1})\n",
    "\n",
    "# Extract features\n",
    "df['nb_dots'] = df['url'].apply(lambda x: x.count('.'))\n",
    "df['nb_slashes'] = df['url'].apply(lambda x: x.count('/'))\n",
    "df['nb_hyphens'] = df['url'].apply(lambda x: x.count('-'))\n",
    "\n",
    "X = df[['nb_dots', 'nb_slashes', 'nb_hyphens']]\n",
    "y = df['label']\n",
    "df[['nb_dots', 'nb_slashes', 'nb_hyphens','label']].sample(2)\n",
    "print(\"label balance check\")\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Train samples: 9144\n",
      "label\n",
      "0    4572\n",
      "1    4572\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Num of Test samples: 2286\n",
      "label\n",
      "1    1143\n",
      "0    1143\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Stratified train-test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"Num of Train samples: {len(y_train)}\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nNum of Test samples: {len(y_test)}\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Logistic Regression\n",
    "# (Needs Scaling)\n",
    "# ============================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "# ============================\n",
    "# GaussianNB\n",
    "# ============================\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "\n",
    "# ============================\n",
    "# 6. MultinomialNB\n",
    "# ============================\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: ltr; text-align: left; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "# URL Phishing Detection â€“ Feature Engineering and Model Evaluation\n",
    "\n",
    "## 1. Extracted Features\n",
    "From each URL, the following three numerical features were extracted:\n",
    "\n",
    "- **nb_dots** â€“ number of \".\" characters  \n",
    "- **nb_slashes** â€“ number of \"/\" characters  \n",
    "- **nb_hyphens** â€“ number of \"-\" characters  \n",
    "\n",
    "These structural URL-based features are simple but useful for distinguishing phishing from legitimate URLs.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Trainâ€“Test Split\n",
    "The dataset was divided using an **80/20 stratified split** to preserve the original class balance of:\n",
    "\n",
    "- 0 â†’ legitimate  \n",
    "- 1 â†’ phishing  \n",
    "\n",
    "Stratification ensures the same ratio of classes in both train and test sets.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Normalization â€“ Is It Necessary?\n",
    "Yes â€” **normalization is necessary for Logistic Regression**, because it is a distance-based linear model and is sensitive to the scale of features.\n",
    "\n",
    "### Applied method:\n",
    "- **StandardScaler (Z-score normalization)**  \n",
    "  $$\n",
    "  x_{norm} = \\frac{x - \\mu}{\\sigma}\n",
    "  $$\n",
    "\n",
    "### For Naive Bayes:\n",
    "- **GaussianNB**: does *not* require scaling  \n",
    "- **MultinomialNB**: should *not* be scaled (requires non-negative discrete counts)\n",
    "\n",
    "Therefore:\n",
    "- Logistic Regression â†’ **scaled**  \n",
    "- GaussianNB â†’ **not scaled**  \n",
    "- MultinomialNB â†’ **not scaled**\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Which Naive Bayes Variant Is Suitable?\n",
    "Between **GaussianNB** and **MultinomialNB**:\n",
    "\n",
    "### âœ” GaussianNB is theoretically the correct choice  \n",
    "Because:\n",
    "- our features (`nb_dots`, `nb_slashes`, `nb_hyphens`) are **continuous numeric counts**,  \n",
    "- GaussianNB assumes **continuous features drawn from a normal distribution**,  \n",
    "- MultinomialNB assumes **discrete word counts from text**, not general numeric features.\n",
    "\n",
    "### However:\n",
    "Both models were tested experimentally.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Model Results\n",
    "\n",
    "| Model                | Accuracy | Precision | Recall | F1 Score |\n",
    "|----------------------|----------|-----------|--------|----------|\n",
    "| **Logistic Regression** | **0.6763** | 0.7042 | 0.6080 | **0.6526** |\n",
    "| **Multinomial NB**      | 0.5424 | 0.5286 | **0.7830** | 0.6312 |\n",
    "| **Gaussian NB**         | **0.6824** | **0.8243** | 0.4637 | 0.5935 |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Interpretation\n",
    "\n",
    "### Logistic Regression\n",
    "- Balanced performance.  \n",
    "- Highest **F1 score**, making it the most reliable overall.  \n",
    "- Scaling improved stability.\n",
    "\n",
    "### Multinomial NB\n",
    "- Lowest accuracy, but **highest recall** â†’ detects more phishing URLs.  \n",
    "- Useful when false negatives (missed phishing) are more dangerous.\n",
    "\n",
    "### Gaussian NB\n",
    "- Highest precision, good accuracy, but low recall.  \n",
    "- More conservative â†’ often predicts â€œlegitimateâ€.  \n",
    "- Matches the nature of our numeric features but suffers from small dataset size.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Final Conclusion\n",
    "\n",
    "- **Best overall model â†’ Logistic Regression**  \n",
    "  It provides the strongest balance between detecting phishing and avoiding false alarms.\n",
    "\n",
    "- **Best for maximum phishing detection â†’ Multinomial NB**  \n",
    "  Highest recall.\n",
    "\n",
    "- **Best for minimizing false positives â†’ Gaussian NB**  \n",
    "  Highest precision.\n",
    "\n",
    "GaussianNB is the *theoretically correct* variant for this dataset,  \n",
    "but Logistic Regression delivered the **best practical performance**.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: ltr; text-align: left; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "# URL Phishing Detection â€“ Final Model Comparison\n",
    "\n",
    "After extracting the numerical URL features (`nb_dots`, `nb_slashes`, `nb_hyphens`) and performing an 80/20 stratified split, three models were trained:\n",
    "\n",
    "- **Logistic Regression**\n",
    "- **Multinomial Naive Bayes**\n",
    "- **Gaussian Naive Bayes**\n",
    "\n",
    "The following evaluation metrics were computed on the test set:\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Model Performance\n",
    "\n",
    "| Model               | Accuracy | Precision | Recall | F1 Score |\n",
    "|---------------------|----------|-----------|--------|----------|\n",
    "| **Logistic Regression** | **0.6763** | 0.7042 | 0.6080 | 0.6526 |\n",
    "| **Multinomial NB**      | 0.5424 | 0.5286 | **0.7830** | 0.6312 |\n",
    "| **Gaussian NB**         | **0.6824** | **0.8243** | 0.4637 | 0.5935 |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Interpretation\n",
    "\n",
    "### Logistic Regression\n",
    "- Balanced performance.\n",
    "- Good precision and decent recall.\n",
    "- Strong overall F1 score â†’ best compromise between false positives and false negatives.\n",
    "\n",
    "### Multinomial NB\n",
    "- Lowest accuracy, but **highest recall**, meaning it catches more phishing samples.\n",
    "- Useful when missing a phishing URL is more dangerous than misclassifying a legitimate one.\n",
    "\n",
    "### Gaussian NB\n",
    "- Highest accuracy and precision.\n",
    "- But recall is low â†’ tends to miss phishing URLs.\n",
    "- Indicates the model is conservative and predicts â€œlegitimateâ€ more often.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Conclusion\n",
    "\n",
    "- **Best overall model:** Logistic Regression  \n",
    "  (highest F1 among balanced models)\n",
    "\n",
    "- **Best for catching phishing URLs:** Multinomial NB  \n",
    "  (highest recall)\n",
    "\n",
    "- **Best for avoiding false alarms:** Gaussian NB  \n",
    "  (highest precision)\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Accuracy  Precision  Recall  F1 Score\n",
      "Logistic Regression    0.6763     0.7042  0.6080    0.6526\n",
      "     Multinomial NB    0.5424     0.5286  0.7830    0.6312\n",
      "        Gaussian NB    0.6824     0.8243  0.4637    0.5935\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Evaluation Function\n",
    "# ============================\n",
    "def evaluate(name, y_true, y_pred):\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": round(accuracy_score(y_true, y_pred), 4),\n",
    "        \"Precision\": round(precision_score(y_true, y_pred), 4),\n",
    "        \"Recall\": round(recall_score(y_true, y_pred), 4),\n",
    "        \"F1 Score\": round(f1_score(y_true, y_pred), 4)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "results.append(evaluate(\"Logistic Regression\", y_test, y_pred_lr))\n",
    "results.append(evaluate(\"Multinomial NB\", y_test, y_pred_mnb))\n",
    "results.append(evaluate(\"Gaussian NB\", y_test, y_pred_gnb))\n",
    "\n",
    "# ============================\n",
    "# 8. Beautiful Result Table\n",
    "# ============================\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>nb_slashes</th>\n",
       "      <th>nb_hyphens</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nb_dots  nb_slashes  nb_hyphens  status\n",
       "0        3           3           0       0\n",
       "1        1           5           0       1\n",
       "2        4           5           1       1\n",
       "3        2           2           0       0\n",
       "4        2           5           2       0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_temp = X.copy()\n",
    "X_temp[\"status\"]=y\n",
    "X_temp.head(5)\n",
    "# del X_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø¯ÙˆÙ…: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§Ø² Ø¢Ø¯Ø±Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "- Ø·ÙˆÙ„ Ú©Ù„ Ø¢Ø¯Ø±Ø³ (length_url)\n",
    "<br>\n",
    "- Ù†Ø³Ø¨Øª ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±Ù‚Ø§Ù… (0â€“9) Ø¨Ù‡ Ú©Ù„ Ø·ÙˆÙ„ Ø¢Ø¯Ø±Ø³ (ratio_digits_url)\n",
    "<br>\n",
    "- Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† Ú©Ù„Ù…Ù‡ (Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ù„ÙØ¨Ø§ÛŒÛŒ) Ø¯Ø± URL Ú©Ù‡ Ø¨Ø§ Ø¹Ù„Ø§Ø¦Ù… Ø¬Ø¯Ø§Ø³Ø§Ø² (Ù†Ù‚Ø·Ù‡ØŒ Ø¹Ù„Ø§Ù…Øªâ€ŒØ³ÙˆØ§Ù„ØŒ Ø§Ø³Ù„Ø´ Ùˆ...) Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. (longest_words_raw)\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§ÛŒÙ† Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ø´ØªØ§Ø¯ Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´) Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ† (Ø¨ÛŒØ³Øª Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§) Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Naive Bayes Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³ØªØŸ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯. Ú©Ø¯Ø§Ù… Ù†Ø³Ø®Ù‡ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³ØªØŸ \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø¯ÙˆÙ…:</b><br>\n",
    "{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_url</th>\n",
       "      <th>ratio_digits_url</th>\n",
       "      <th>longest_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126</td>\n",
       "      <td>0.150794</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length_url  ratio_digits_url  longest_words  label\n",
       "0          37          0.000000             11      0\n",
       "1          77          0.220779             19      1\n",
       "2         126          0.150794             13      1\n",
       "3          18          0.000000              5      0\n",
       "4          55          0.000000             11      0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"datasets/q2/urls.csv\")\n",
    "\n",
    "# Convert labels\n",
    "df['label'] = df['status'].map({'legitimate':0, 'phishing':1})\n",
    "\n",
    "# Feature 1: URL length (apply log1p)\n",
    "df['length_url'] = df['url'].apply(len)\n",
    "df['length_url_log'] = np.log1p(df['length_url'])\n",
    "\n",
    "# Feature 2: ratio of digits\n",
    "df['ratio_digits_url'] = df['url'].apply(lambda x: sum(c.isdigit() for c in x)/len(x))\n",
    "\n",
    "# Feature 3: longest alphabetic word\n",
    "def longest_word(url):\n",
    "    words = re.split(r'[./?&=-]', url)\n",
    "    words_alpha = [w for w in words if w.isalpha()]\n",
    "    return max([len(w) for w in words_alpha], default=0)\n",
    "df['longest_words'] = df['url'].apply(longest_word)\n",
    "df['longest_words_log'] = np.log1p(df['longest_words'])\n",
    "\n",
    "\n",
    "df[['length_url', 'ratio_digits_url', 'longest_words','label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using log of length may help\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_url_log</th>\n",
       "      <th>ratio_digits_url</th>\n",
       "      <th>longest_words_log</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.637586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.356709</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.844187</td>\n",
       "      <td>0.150794</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.025352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length_url_log  ratio_digits_url  longest_words_log  label\n",
       "0        3.637586          0.000000           2.484907      0\n",
       "1        4.356709          0.220779           2.995732      1\n",
       "2        4.844187          0.150794           2.639057      1\n",
       "3        2.944439          0.000000           1.791759      0\n",
       "4        4.025352          0.000000           2.484907      0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"using log of length may help\")\n",
    "df[['length_url_log', 'ratio_digits_url', 'longest_words_log','label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Define feature sets\n",
    "# ----------------------------\n",
    "no_log = ['length_url','ratio_digits_url','longest_words']\n",
    "yes_log = ['length_url_log','ratio_digits_url','longest_words_log']\n",
    "\n",
    "# ----------------------------\n",
    "# Train/Test Split (stratified)\n",
    "# ----------------------------\n",
    "# No-log features\n",
    "X_no_log = df[no_log]\n",
    "y = df['label']\n",
    "X_train_no_log, X_test_no_log, y_train, y_test = train_test_split(\n",
    "    X_no_log, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Log features\n",
    "X_log = df[yes_log]\n",
    "X_train_log, X_test_log, _, _ = train_test_split(\n",
    "    X_log, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Scaling for Logistic Regression\n",
    "# ----------------------------\n",
    "scaler_no_log = StandardScaler()\n",
    "X_train_scaled_no_log = scaler_no_log.fit_transform(X_train_no_log)\n",
    "X_test_scaled_no_log = scaler_no_log.transform(X_test_no_log)\n",
    "\n",
    "scaler_log = StandardScaler()\n",
    "X_train_scaled_log = scaler_log.fit_transform(X_train_log)\n",
    "X_test_scaled_log = scaler_log.transform(X_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results without log features ===\n",
      "              Model  Accuracy  Precision  Recall  F1 Score\n",
      "Logistic Regression    0.6500     0.6974  0.5302    0.6024\n",
      "        Gaussian NB    0.6382     0.7565  0.4077    0.5298\n",
      "     Multinomial NB    0.5875     0.6006  0.5223    0.5587\n",
      "\n",
      "=== Results with log features ===\n",
      "              Model  Accuracy  Precision  Recall  F1 Score\n",
      "Logistic Regression    0.6452     0.6861  0.5354    0.6015\n",
      "        Gaussian NB    0.6369     0.7220  0.4453    0.5509\n",
      "     Multinomial NB    0.6479     0.7076  0.5039    0.5887\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Models\n",
    "# ----------------------------\n",
    "def evaluate_models(X_train_scaled, X_test_scaled, X_train_raw, X_test_raw, y_train, y_test):\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "    # Gaussian NB\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train_raw, y_train)\n",
    "    y_pred_gnb = gnb.predict(X_test_raw)\n",
    "\n",
    "    # Multinomial NB (not recommended)\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train_raw, y_train)\n",
    "    y_pred_mnb = mnb.predict(X_test_raw)\n",
    "\n",
    "    # Evaluation\n",
    "    def eval_model(name, y_true, y_pred):\n",
    "        return {\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": round(accuracy_score(y_true, y_pred),4),\n",
    "            \"Precision\": round(precision_score(y_true, y_pred),4),\n",
    "            \"Recall\": round(recall_score(y_true, y_pred),4),\n",
    "            \"F1 Score\": round(f1_score(y_true, y_pred),4)\n",
    "        }\n",
    "\n",
    "    results = []\n",
    "    results.append(eval_model(\"Logistic Regression\", y_test, y_pred_lr))\n",
    "    results.append(eval_model(\"Gaussian NB\", y_test, y_pred_gnb))\n",
    "    results.append(eval_model(\"Multinomial NB\", y_test, y_pred_mnb))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate no-log features\n",
    "results_no_log = evaluate_models(\n",
    "    X_train_scaled_no_log, X_test_scaled_no_log,\n",
    "    X_train_no_log, X_test_no_log,\n",
    "    y_train, y_test\n",
    ")\n",
    "\n",
    "# Evaluate log features\n",
    "results_log = evaluate_models(\n",
    "    X_train_scaled_log, X_test_scaled_log,\n",
    "    X_train_log, X_test_log,\n",
    "    y_train, y_test\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"=== Results without log features ===\")\n",
    "print(results_no_log.to_string(index=False))\n",
    "print(\"\\n=== Results with log features ===\")\n",
    "print(results_log.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø³ÙˆÙ…: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø² Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø§ÛŒÛŒØ¯ Ùˆ Ù…Ø´Ø§Ø¨Ù‡ Ø¯Ùˆ Ø¨Ø®Ø´ Ù‚Ø¨Ù„ÛŒØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ Ú†Ø±Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± ØªØ´Ø®ÛŒØµ ÙÛŒØ´ÛŒÙ†Ú¯ Ù…Ø¤Ø«Ø± Ø¨Ø§Ø´Ø¯ØŸ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø³ÙˆÙ…:</b><br>\n",
    "{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_https</th>\n",
       "      <th>has_www</th>\n",
       "      <th>length_after_last_slash</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8439</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5444</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      has_https  has_www  length_after_last_slash  label\n",
       "8439          0        0                 2.302585      1\n",
       "5444          0        1                 3.218876      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"datasets/q2/urls.csv\")\n",
    "\n",
    "# Convert labels\n",
    "df['label'] = df['status'].map({'legitimate':0, 'phishing':1})\n",
    "\n",
    "# Features\n",
    "df['has_https'] = df['url'].apply(lambda x: 1 if x.startswith(\"https://\") else 0)\n",
    "df['has_www'] = df['url'].apply(lambda x: 1 if \"://www.\" in x else 0)\n",
    "\n",
    "def length_after_last_slash(url):\n",
    "    # remove trailing slash (to avoid empty result)\n",
    "    url = url.rstrip('/')\n",
    "    # split and take the last segment\n",
    "    last_part = url.split('/')[-1]\n",
    "    return  np.log1p(len(last_part))\n",
    "\n",
    "df['length_after_last_slash'] = df['url'].apply(length_after_last_slash)\n",
    "X = df[['has_https', 'has_www', 'length_after_last_slash']]\n",
    "y = df['label']\n",
    "df[['has_https', 'has_www', 'length_after_last_slash','label']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Train/Test Split (stratified)\n",
    "# ----------------------------\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "# ----------------------------\n",
    "# Scaling for Logistic Regression\n",
    "# ----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results with log features ===\n",
      "              Model  Accuracy  Precision  Recall  F1 Score\n",
      "Logistic Regression    0.7297     0.7033  0.7944    0.7461\n",
      "        Gaussian NB    0.7358     0.7016  0.8206    0.7565\n",
      "     Multinomial NB    0.7297     0.7033  0.7944    0.7461\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Models\n",
    "# ----------------------------\n",
    "def evaluate_models(X_train_scaled, X_test_scaled, X_train_raw, X_test_raw, y_train, y_test):\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "    # Gaussian NB\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train_raw, y_train)\n",
    "    y_pred_gnb = gnb.predict(X_test_raw)\n",
    "\n",
    "    # Multinomial NB (not recommended)\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train_raw, y_train)\n",
    "    y_pred_mnb = mnb.predict(X_test_raw)\n",
    "\n",
    "    # Evaluation\n",
    "    def eval_model(name, y_true, y_pred):\n",
    "        return {\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": round(accuracy_score(y_true, y_pred),4),\n",
    "            \"Precision\": round(precision_score(y_true, y_pred),4),\n",
    "            \"Recall\": round(recall_score(y_true, y_pred),4),\n",
    "            \"F1 Score\": round(f1_score(y_true, y_pred),4)\n",
    "        }\n",
    "\n",
    "    results = []\n",
    "    results.append(eval_model(\"Logistic Regression\", y_test, y_pred_lr))\n",
    "    results.append(eval_model(\"Gaussian NB\", y_test, y_pred_gnb))\n",
    "    results.append(eval_model(\"Multinomial NB\", y_test, y_pred_mnb))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate no-log features\n",
    "results = evaluate_models(\n",
    "    X_train_scaled, X_test_scaled,\n",
    "    X_train, X_test,\n",
    "    y_train, y_test\n",
    ")\n",
    "print(\"\\n=== Results with log features ===\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ØªÙ…Ø§Ù… Û¹ ÙˆÛŒÚ˜Ú¯ÛŒ (Û³ Ø³Ø§Ø®ØªØ§Ø±ÛŒ + Û³ Ø¢Ù…Ø§Ø±ÛŒ + Û³ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡) Ø±Ø§ Ø¨Ø§ Ù‡Ù… ØªØ±Ú©ÛŒØ¨ Ú©Ù†ÛŒØ¯.\n",
    "Ø³Ù¾Ø³ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ Ø±Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø¢ÛŒØ§ ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§Ø¹Ø« Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ\n",
    "<br>\n",
    "Ø§Ú¯Ø± Ø®ÛŒØ±ØŒ Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø¯Ù„ÛŒÙ„ Ø¢Ù† Ú†ÛŒØ³ØªØŸ (Ù…Ø«Ù„Ø§Ù‹ ØªØ¯Ø§Ø®Ù„ Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ÛŒØ§ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ù„Ø§ Ùˆ...)\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"datasets/q2/urls.csv\")\n",
    "\n",
    "# Convert labels\n",
    "df['label'] = df['status'].map({'legitimate':0, 'phishing':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11430 entries, 0 to 11429\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   has_https                11430 non-null  int64  \n",
      " 1   has_www                  11430 non-null  int64  \n",
      " 2   length_after_last_slash  11430 non-null  float64\n",
      " 3   nb_dots                  11430 non-null  int64  \n",
      " 4   nb_slashes               11430 non-null  int64  \n",
      " 5   nb_hyphens               11430 non-null  int64  \n",
      " 6   length_url               11430 non-null  float64\n",
      " 7   ratio_digits_url         11430 non-null  float64\n",
      " 8   longest_words            11430 non-null  float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 803.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Features (custom)\n",
    "df['has_https'] = df['url'].apply(lambda x: 1 if x.startswith(\"https://\") else 0)\n",
    "df['has_www'] = df['url'].apply(lambda x: 1 if \"://www.\" in x else 0)\n",
    "\n",
    "def length_after_last_slash(url):\n",
    "    # remove trailing slash (to avoid empty result)\n",
    "    url = url.rstrip('/')\n",
    "    # split and take the last segment\n",
    "    last_part = url.split('/')[-1]\n",
    "    return  np.log1p(len(last_part))\n",
    "\n",
    "df['length_after_last_slash'] = df['url'].apply(length_after_last_slash)\n",
    "\n",
    "# Features (structural)\n",
    "df['nb_dots'] = df['url'].apply(lambda x: x.count('.'))\n",
    "df['nb_slashes'] = df['url'].apply(lambda x: x.count('/'))\n",
    "df['nb_hyphens'] = df['url'].apply(lambda x: x.count('-'))\n",
    "\n",
    "# Features (statistical)\n",
    "df['length_url'] = np.log1p(df['url'].apply(len))\n",
    "df['ratio_digits_url'] = df['url'].apply(lambda x: sum(c.isdigit() for c in x)/len(x))\n",
    "def longest_word(url):\n",
    "    words = re.split(r'[./?&=-]', url)\n",
    "    words_alpha = [w for w in words if w.isalpha()]\n",
    "    return np.log1p(max([len(w) for w in words_alpha], default=0))\n",
    "df['longest_words'] = df['url'].apply(longest_word)\n",
    "\n",
    "X = df[[\"has_https\",\"has_www\",\"length_after_last_slash\",\"nb_dots\",\"nb_slashes\",\"nb_hyphens\",\"length_url\",\"ratio_digits_url\",\"longest_words\"]]\n",
    "y = df[\"label\"]\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_https</th>\n",
       "      <th>has_www</th>\n",
       "      <th>length_after_last_slash</th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>nb_slashes</th>\n",
       "      <th>nb_hyphens</th>\n",
       "      <th>length_url</th>\n",
       "      <th>ratio_digits_url</th>\n",
       "      <th>longest_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.484907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4.356709</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>2.995732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.891820</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4.844187</td>\n",
       "      <td>0.150794</td>\n",
       "      <td>2.639057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4.025352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.484907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   has_https  has_www  length_after_last_slash  nb_dots  nb_slashes  \\\n",
       "0          0        1                 2.397895        3           3   \n",
       "1          0        0                 3.496508        1           5   \n",
       "2          1        0                 3.891820        4           5   \n",
       "3          0        0                 2.484907        2           2   \n",
       "4          0        1                 3.218876        2           5   \n",
       "\n",
       "   nb_hyphens  length_url  ratio_digits_url  longest_words  \n",
       "0           0    3.637586          0.000000       2.484907  \n",
       "1           0    4.356709          0.220779       2.995732  \n",
       "2           1    4.844187          0.150794       2.639057  \n",
       "3           0    2.944439          0.000000       1.791759  \n",
       "4           2    4.025352          0.000000       2.484907  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Train/Test Split (stratified)\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "# ----------------------------\n",
    "# Scaling for Logistic Regression\n",
    "# ----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results ===\n",
      "              Model  Accuracy  Precision  Recall  F1 Score\n",
      "Logistic Regression    0.7848     0.7747  0.8031    0.7887\n",
      "        Gaussian NB    0.7463     0.8285  0.6212    0.7100\n",
      "     Multinomial NB    0.7498     0.7260  0.8023    0.7623\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Models\n",
    "# ----------------------------\n",
    "def evaluate_models(X_train_scaled, X_test_scaled, X_train_raw, X_test_raw, y_train, y_test):\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "    # Gaussian NB\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train_raw, y_train)\n",
    "    y_pred_gnb = gnb.predict(X_test_raw)\n",
    "\n",
    "    # Multinomial NB (not recommended)\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train_raw, y_train)\n",
    "    y_pred_mnb = mnb.predict(X_test_raw)\n",
    "\n",
    "    # Evaluation\n",
    "    def eval_model(name, y_true, y_pred):\n",
    "        return {\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": round(accuracy_score(y_true, y_pred),4),\n",
    "            \"Precision\": round(precision_score(y_true, y_pred),4),\n",
    "            \"Recall\": round(recall_score(y_true, y_pred),4),\n",
    "            \"F1 Score\": round(f1_score(y_true, y_pred),4)\n",
    "        }\n",
    "\n",
    "    results = []\n",
    "    results.append(eval_model(\"Logistic Regression\", y_test, y_pred_lr))\n",
    "    results.append(eval_model(\"Gaussian NB\", y_test, y_pred_gnb))\n",
    "    results.append(eval_model(\"Multinomial NB\", y_test, y_pred_mnb))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate no-log features\n",
    "results = evaluate_models(\n",
    "    X_train_scaled, X_test_scaled,\n",
    "    X_train, X_test,\n",
    "    y_train, y_test\n",
    ")\n",
    "print(\"\\n=== Results ===\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:</b><br>\n",
    "{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø² Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ Ø¨Ø§ÛŒØ¯ \"Ø­Ø¯Ø§Ù‚Ù„\" Ø¨Ù‡ Ø³ÙˆØ§Ù„â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯:\n",
    "<br>\n",
    "- Ú©Ø¯Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³ØªØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- Ú©Ø¯Ø§Ù… Ù†ÙˆØ¹ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ£Ø«ÛŒØ± Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØŸ Ú©Ø¯Ø§Ù… Ù†ÙˆØ¹ Ú©Ù…ØªØ±ÛŒÙ†ØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†ÛŒØ§Ø² Ø¨ÙˆØ¯Ù‡ Ø§Ø³ØªØŸ Ø§Ú¯Ø± Ø¨Ù„Ù‡ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- ØºÛŒØ± Ø§Ø² Ø§Ø³ØªØ®Ø±Ø§Ø­ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø² Ø®ÙˆØ¯ Ø¢Ø¯Ø±Ø³ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒØŒ Ú†Ù‡ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø¯ÛŒÚ¯Ø±ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§ØªØ®Ø§Ø° Ù†Ù…ÙˆØ¯ØŸ\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…:</b><br>\n",
    "{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØ­ÙˆÛŒÙ„**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ±Ù…Øª Ø²ÛŒØ± Ù†Ø§Ù…Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆØ¯: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">Ù†Ø­ÙˆÙ‡ Ø§Ù†Ø¬Ø§Ù… ØªÙ…Ø±ÛŒÙ†:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ <code>WRITE YOUR CODE HERE</code> Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯.</li>\n",
    "  <li>Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒØŒ Ù…ØªÙ† <code>{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}</code> Ø±Ø§ Ø¨Ø§ Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>Ù…Ø§ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´Ø®ØµÛŒ Ø§Ø² Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯. Ø§ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ú©Ù‡ Ú©Ø¯ÛŒ Ú©Ù‡ Ù†ÙˆØ´ØªÛŒØ¯ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØµØ­ÛŒØ­ Ø±Ø§ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø®ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ú©Ø¯ÛŒ Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¢Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯ ØªØ­ÙˆÛŒÙ„ Ø¯Ù‡ÛŒØ¯ØŒ Ø§ÛŒÙ† ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> <li>Ù…Ø§ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø±Ù‚Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÙ„Ø¨ Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø¯. Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯ Ø§Ø² Ø¯ÛŒÚ¯Ø±Ø§Ù† Ù†ÛŒØ² ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØªÙˆØ¶ÛŒØ­Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ùˆ Ø¯Ù‚Øª Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³Øª. Ø¨Ù‡ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§ØºØ°ÛŒ ØªØ­ÙˆÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ ÛŒØ§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ú©Ø³ Ø¯Ø± Ø³Ø§ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆÙ†Ø¯ØŒ ØªØ±ØªÛŒØ¨ Ø§Ø«Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.</li>\n",
    "<li>\n",
    " Ù‡Ù…Ù‡â€ŒÛŒ Ú©Ø¯Ù‡Ø§ÛŒ Ù¾ÛŒÙˆØ³Øª Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ÛŒØ³ØªÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ù…Ø¬Ø¯Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®Ø§ØµÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ø¨Ø§ÛŒØ³ØªÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù†ÛŒØ² Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.  Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ Ú©Ù‡  ØªÙ…Ø§Ù…ÛŒ Ú©Ø¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ ØªÙˆØ³Ø· Ø´Ù…Ø§ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ú©Ø¯Ù‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯. Ø¨Ù‡ Ú©Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ù†Ø¨Ø§Ø´Ø¯ Ù†Ù…Ø±Ù‡â€ŒØ§ÛŒ ØªØ¹Ù„Ù‚ Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "</li>\n",
    "<li>ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª ØªÚ©â€ŒÙ†ÙØ±Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ÛŒØ¯ Ù†ØªÛŒØ¬Ù‡ ÙØ¹Ø§Ù„ÛŒØª ÙØ±Ø¯ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (Ù‡Ù…ÙÚ©Ø±ÛŒ Ùˆ Ø¨Ù‡ Ø§ØªÙØ§Ù‚ Ù‡Ù… Ù†ÙˆØ´ØªÙ† ØªÙ…Ø±ÛŒÙ† Ù†ÛŒØ² Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³Øª). Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ø§Ù‡Ø¯Ù‡\n",
    " ØªØ´Ø§Ø¨Ù‡ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÚ©Ù†Ù†Ø¯Ù‡ØŒ Ù†Ù…Ø±Ù‡ ØªÙ…Ø±ÛŒÙ† ØµÙØ± Ùˆ Ø¨Ù‡ Ø§Ø³ØªØ§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "Ù„Ø·ÙØ§Ù‹ ØªÙ…Ø§Ù…ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ <b>ÙÙˆÙ†Øª ÙˆØ²ÛŒØ± (Vazir)</b> Ùˆ Ø¨Ù‡â€ŒØµÙˆØ±Øª <b>Ø±Ø§Ø³Øªâ€ŒÚ†ÛŒÙ†</b> Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÙˆÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¸Ø§Ù‡Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ ÛŒÚ©â€ŒØ¯Ø³Øª Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯.  \n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØªØ´Ø±ÛŒØ­ÛŒØŒ Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ú©Ø§Ù…Ù„ØŒ Ù…Ù†Ø³Ø¬Ù… Ùˆ Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù†Ú¯Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ù‡ Ú†ÛŒÙ†Ø´ ØªÙ…ÛŒØ² Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø±Ø³Øª Ú©Ø¯Ù‡Ø§ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§ Ø¨Ø§ ÙØ±Ù…Øª Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ùˆ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´ÙˆØ¯.\n",
    "</li>\n",
    " <li>Ø¨Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ ÙØ±Ù…Øª Markdown Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">Ø§ÛŒÙ† Ù„ÛŒÙ†Ú©</a> Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
